---
title: "Homework 1"
author: "Marco Sciorilli \\ Ilaria Rochelli \\ Francesco Tomba \\ Lorenzo Cavuoti"
date: "05/11/2020"
output:
  pdf_document: default
  html_document:
    df_cat: paged
institute: University of Trieste
subtitle: Statical Methods for Data Science
fontsize: 10pt
header-includes: \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1.1
Exponential random variable, $X \geq 0$ , has p.d.f. $f (x) = \lambda e^{-\lambda x}$  

#### 1) c.d.f and quantile function:
$$ F_X(x) = \int_0^x f(t)dt = \lambda  \int_0^x e^{-\lambda t} dt = 1-e^{-\lambda x} $$
$$ F^{-1}(p) = \min(x|F(x) \geq p) \qquad p = F(x) = 1-e^{-\lambda x}\\
\ln(e^{-\lambda x}) = \ln(1-F(X)) \Rightarrow x = -\ln(1-F(X)) / \lambda \\ $$

$$\Rightarrow \;F^{-1}(p) = -\frac{\ln(1-p)}{\lambda}$$

#### 2) Pr(X < $\lambda$) and median:
$$ \mathrm {Pr}(X < \lambda) = \mathrm {Pr}(X \leq \lambda) - \mathrm {Pr}(X = \lambda)
= F_X(\lambda) - 0 = 1-e^{-\lambda^2} $$
Median: 
$$ \mathrm {Pr}(X < Me) = F_X(Me) = 0.5 \; \Rightarrow \; Me = F^{-1}(0.5) =
\ln(2)/\lambda$$  

#### 3) Mean and variance:
We first find the m.g.f. of X from which we compute the first two moments, given those we find the mean and variance of X
$$ M_X(t) = \mathrm E[e^{tx}] = \int_0^\infty e^{tx} \lambda e^{-\lambda x} dx =
\frac{\lambda}{\lambda - t} $$
$$ \mathrm E[X^k] = \frac{d^k M_X}{dt^k}\bigg|_{t=0} \; \Rightarrow \; 
\mathrm E[X] = 1/\lambda \quad \mathrm E[X^2] = 2/\lambda^2 $$
$$ \mu = \mathrm E[X] = 1/\lambda \qquad 
\mathrm {Var}(X) = \mathrm E[X]^2 - \mathrm E[X^2] = 1/\lambda^2$$

## Exercise 1.2
Evaluate $\mathrm{Pr} (X < 0.5, Y < 0.5)$ if X and Y have joint p.d.f. 
$\; f(x, y) = x + 3/2 y^2 \quad x \in [0,1] \; y \in [0,1]\;$ and $f=0$ elsewhere
$$ \mathrm{Pr} (X < 0.5, Y < 0.5) = \int_0^{1/2}\int_0^{1/2} x + \frac{3}{2} y^2 \; dydx = 
\int_0^{1/2} \frac{1}{2}x + \frac{1}{16} \; dx = \frac{1}{16} + \frac{1}{32} =
\frac{3}{32} $$


## Exercise 1.6
Let $X$ and $Y$ be non-independent random variables, such that 
$\mathrm{Var}(X) =\sigma_x^2 \;, \mathrm{Var}(Y) =\sigma_y^2 \;, \\ \mathrm{Cov}(X, Y) =\sigma_{x,y}^2$ find $\mathrm{Var}(X + Y)$ and $\mathrm{Var}(X - Y)$

This problem is equivalent to finding the diagonal of the covariance matrix of the linear transformation $(x', y') \rightarrow (x+y, x-y)$

$$\begin{pmatrix}
x'\\
y' 
\end{pmatrix} =
\begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix}
\begin{pmatrix}
x\\
y
\end{pmatrix} = 
\begin{pmatrix}
x+y\\
x-y
\end{pmatrix} \qquad
A = \begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix} \quad
\Sigma=\begin{pmatrix}
\sigma_x^2 & \sigma_{xy}^2\\
\sigma_{xy}^2 & \sigma_y^2
\end{pmatrix}$$

$$\mathrm{Cov}(X', Y')=A\Sigma A^T=
\begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix}
\begin{pmatrix}
\sigma_x^2 & \sigma_{xy}^2\\
\sigma_{xy}^2 & \sigma_y^2
\end{pmatrix}
\begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix}=
\begin{pmatrix}
\sigma_x^2+2\sigma_{xy}^2+\sigma_y^2  & \sigma_x^2-\sigma_y^2\\
\sigma_x^2-\sigma_y^2 & \sigma_x^2-2\sigma_{xy}^2+\sigma_y^2
\end{pmatrix}$$

$$\Rightarrow \mathrm{Var}(X + Y) = \sigma_x^2+2\sigma_{xy}^2+\sigma_y^2 \qquad
\mathrm{Var}(X - Y) = \sigma_x^2-2\sigma_{xy}^2+\sigma_y^2$$

## Exercise 1.8
If $\ln(X) \sim N(\mu, \sigma^2)$, find the p.d.f. of X.  
Let's define $Y=\ln(X)\sim N(\mu,\sigma^2)$

$$ f_Y(y)  =\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2} \qquad Y = \ln(X) \Rightarrow X = e^Y = g(Y)$$
and
$$ f_X(x) = f_Y\{ g^{-1}(x)\}\Big|\frac{dY}{dx}\Big| =
f_Y\{ \ln(x)\}\Big| \frac{d\ln(x)}{dx}\Big|=\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{\ln(x)-\mu}{\sigma}\right)^2}\cdot \Big|\frac{1}{x} \Big| =
\frac{1}{x\sigma \sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{\ln(x)-\mu}{\sigma}\right)^2} $$
In the last step we discarded the absolute value since x>0

## Exercise 3.3
Rewrite the function f1, replacing the loop with efficient code:
```{r echo=TRUE}
f1 <- function(n) {
  set.seed(0)
  z <- rnorm(n)
  zneg <- 0; j <- 1
  for (i in 1:n) {
    if (z[i]<0) {
      zneg[j] <- z[i]
      j <- j + 1
    }
  }
  return(zneg)
}

# Rewritten function
f2 <- function(n) {
  set.seed(0)
  z <- rnorm(n)
  return(z[z<0.])
}

n <- 1000000

# Checks if the results are the same
cat("Mean absolute error between function returns:", mean(abs(f1(n) - f2(n))), "\n")

# Runtime of f1
system.time(f1(n))

# Runtime of f2
system.time(f2(n))
```
We see that the rewritten function f2 is about 4 times faster than the original function f1 while giving the same result

## Exercise 3.5
Running time comparison between solving a linear system of equation using the `solve` method to perform the matrix inversion explicitly and the pure `solve` method
```{r echo=TRUE}
# Generate the matrix and the solution vector of the linear system
set.seed(0); n <- 1000
A <- matrix(runif(n*n),n,n); x.true <- runif(n)
y <- A%*%x.true

# Elapsed time solve method with inversion
system.time(
  {A_inv <- solve(A)
  x <- A_inv%*%y})
cat("Mean absolute error:", mean(abs(x-x.true)), "\n\n")

# Elapsed time pure solve method
system.time(x <- solve(A, y))
cat("Mean absolute error:", mean(abs(x-x.true)))
```
We see that `solve` used directly is about 2 times faster and 1 order of magnitude more precise than taking the inverse of the matrix and multiply $A^{-1}\boldsymbol{\mathrm y}$

## Exercise 3.6
Empirical distribution function  

`e_cdf(x, plot.cdf = TRUE)`  
The function e_cdf takes an unordered vector of observations `x` and returns the values of the empirical cdf for each value, in the order corresponding to the original x value.  
If `plot.cdf==TRUE` the function also plots the cdf of the vector, `default=TRUE`

```{r echo=TRUE}
e_cdf <- function(x, plot.cdf = TRUE) {

  # We create the vector which will contain the cdf 
  q <- vector('numeric', length(x))

  # For the values of the cumulative distribution we have to count how many x_k
  # values there are less than x and divide it by the length of the vector
  for(i in 1:length(x)){
    q[i] <- mean(x < x[i])
  }
  
  # If plot.cdf==TRUE also the plot of the cdf should be returned
  if(plot.cdf) {
    # To obtain a plot I have to sort x and q
    plot(sort.int(x), sort.int(q), type="s", xlab = "x", ylab = "y")
  }                                      

  return(q)
}
```

Code to test the function: (forse bisogna fare i grafici piÃ¹ carini)
```{r echo=TRUE}
n <- 100  # Number of extracted samples
k <- 1000 # Number of points to evaluate the true c.d.f
set.seed(0) # Sets the seed of the R random number generator
par(mfrow=c(1,2)) # Allows to show two plots side by side

# Comparison with the uniform distribution
v <- runif(n) 
c <- e_cdf(v)
x <- seq(min(v), max(v), length.out=k)
lines(x, punif(x), type="l", col = 'red')
qqplot(c, punif(v))

# Comparison with the gaussian distribution
v <- rnorm(n)
c <- e_cdf(v)
x <- seq(min(v), max(v), length.out=k)
lines(x, pnorm(x), type="l", col = 'red')
qqplot(c, pnorm(v))
```
  
Since the points in the qqplot lie on a line we see that our function `e_cdf` correctly computes the empirical distribution function
  
## Homework exercise
Based on the following data, conduct a hypothesis test to determine if the average age of a certain population of interest is equal to 45 years:
```{r echo=TRUE}
age <- c(64, 95, 28, 64, 62, 54, 92, 96, 86, 69, 102, 75, 33,
         33, 68, 86, 45, 37, 59, 20, 33, 18, 7, 18, 38, 66,
         45, 66, 80, 69, 58, 44, 41, 70, 25, 51, 71, 68, 13, 38)
```

We test the null hypothesis H0 with a bilateral and unilateral student t test
```{r echo=TRUE}
n = length(age)
mu = mean(age)
mu.std = sqrt(var(age)/n)
t.oss <- (mu-45)/mu.std # Observed t statistic
```

```{r echo=TRUE}
# Bilateral test: mu!=45
cat("p-value:", 2*pt(-t.oss, df = n-1), "\n")
cat("99 percent confidence interval:", c(qt(0.005, df=n-1), qt(0.995, df=n-1))*mu.std + mu, "\n")
```
We accept H0 since the p-value is greater than 0.01  

```{r echo=TRUE}
# Bilateral test: mu<=45
cat("p-value:", pt(-t.oss, df = n-1), "\n")
cat("99 percent confidence interval:", qt(0.01, df=n-1)*mu.std + mu, "\n")
```
We reject H0 since the p-value is less than 0.01  

Plot of the unilateral and bilateral student t test
```{r echo=TRUE}
t_0.995 <- qt(0.995, df=n-1)
t_0.99 <- qt(0.99, df=n-1)

## Unilateral plot
curve(dt(x, df=n-1), xlim=c(-4, 4), main="P.d.f Student's t test, unilateral", lwd=2)
abline(v=t.oss, col=2, lty=2)
abline(v=t_0.99, col=4, lty=2)
abline(h=0)
legend("topleft", c("critical value", "observed value"), col=c(4,2), lty=c(2,2))

# To create the shaded area:
x1 = c(qt(0.99, n-1), seq(qt(0.99, n-1), 5, 0.00001), 1)
y1 = c(0, dt(seq(qt(0.99, n-1), 5, 0.00001), n-1), 0)
polygon(x1, y1, col="pink", border=NA)


## Bilateral plot
curve(dt(x, df=n-1), xlim=c(-4, 4), main="P.d.f Student's t test, bilateral", lwd=2)
abline(v=t.oss, col=2, lty=2)
abline(v=t_0.995, col=4, lty=2)
abline(h=0)
abline(v=-t_0.995, col=4, lty=2)
legend("topleft", c("critical value", "observed value"), col=c(4,2), lty=c(2,2))

# Right shaded area
x2r = c(qt(0.995, n-1), seq(qt(0.995, n-1), 4, 0.00001), 4)
y2r = c(0, dt(seq(qt(0.995, n-1), 4, 0.00001), n-1), 0)
polygon(x2r, y2r, col="pink", border=NA)

# Left shaded area
x2l = c(-4, seq(-4, qt(0.005, n-1), 0.00001), qt(0.005, n-1))
y2l = c(0, dt(seq(-4, qt(0.005, n-1), 0.00001), n-1), 0)
polygon(x2l, y2l, col="pink", border=NA)
```