---
title: "Homework 3"
author: "Ilaria Rochelli, Giulia Marchiori Pietrosanti, Ciro Antonio Mami, Marco Sciorilli"
date: "17/12/2020"
output: html_document
    
institute: University of Trieste
subtitle: Statical Methods for Data Science
fontsize: 10pt
---
## Chapter 6

### Exercise 8

**Request**: Apply the lm.ridge() function to the litters data, using the generalized cross-validation
(GCV) criterion to choose the tuning parameter. (GCV is an approximation to cross-validation.)

* In particular, estimate the coefficients of the model relating brainwt to bodywt and
lsize and compare with the results obtained using lm().

* Using both ridge and ordinary regression, estimate the mean brain weight when litter
size is 10 and body weight is 7. Use the bootstrap, with case-resampling, to compute
approximate 95% percentile confidence inter

**Solution**:

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
library(MASS)
library(DAAG)
```

As first thing we are asked to apply *lm.ridge* to the litters data choosing the best tuning parameter via GCV.

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
data <- litters 

#we estimate the best tuning parameter via GCV
select(lm.ridge(brainwt ~ lsize + bodywt, data=data, lambda=seq(0, 10, 0.001)))

#we apply lm.ridge
fit.gcv <- lm.ridge(brainwt ~ lsize+ bodywt, data=data, lambda=0.118)
```

The coefficients of the ridge model are the following: 

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
fit.gcv
```

Let's compare the results obtained with the ridge model with the results obtained using *lm()*

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
#we fit a linear model using lm
fit.lm <- lm(brainwt ~ lsize + bodywt, data=data) 
fit.lm
```

The mean brain weight when litter size is 10 and body weight is 7, both for *fit.gcv* and *fit.lm* are respectively
```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
c(1, 10, 7) %*% coefficients(fit.gcv)
c(1, 10, 7) %*% coefficients(fit.lm)
```

Now we are asked to compute 95% percentile confidence intervals using both *lm.ridge()* and *lm()*. For the ridge regression we obtain the following values:

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
library(boot)

#we create a function which computes the mean for every bootstrap sample for lm.ridge()
ridge_mean_brain_wt <- function(formula, data, index){
  d <- data[index, ]
  model <- lm.ridge(brainwt ~. , data , lambda = seq(0,10,0.001))
  fit <- lm.ridge(formula, data=d, lambda = which.min(model$GCV))
  boot_mean_ridge <- c(1, 10, 7) %*% coefficients(fit)
  return(boot_mean_ridge)
}

boot_data_ridge <- boot(data=data, statistic=ridge_mean_brain_wt, R=1000, formula=brainwt~lsize+bodywt)
boot.ci(boot_data_ridge, conf.level=0.95, type=c("norm", "basic", "perc", "bca"))
```

While for the linear regression we obtain these:

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
#we repeat the same operation for the linear model
lm_mean_brain_wt <- function(formula, data, index){
  d <- data[index, ]
  fit <- lm(formula, data=d)
  boot_mean_lm <- c(1, 10, 7) %*% coefficients(fit)
  return(boot_mean_lm)
}

#we compute the bootstrap based confidence interval at level 95%
boot_data_lm <- boot(data=data, statistic=lm_mean_brain_wt, R=1000, formula=brainwt~lsize+bodywt)
boot.ci(boot_data_lm, conf.level=0.95, type=c("norm", "basic", "perc", "bca"))
```

We can also compare these results with the ones obtained by using *predict()*:

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
predict.lm(fit.lm, newdata = data.frame(lsize = 10 , bodywt = 7), interval = "confidence", level = 0.95)
```



### Exercise 10

**Request**:The data frame table.b3 in the MPV package contains data on gas mileage and 11 other
variables for a sample of 32 automobiles.

**Solution**:

* Construct a scatterplot of y (mpg) versus x1 (displacement). Is the relationship between
these variables non-linear?

Just by looking at the scatterplot of mpg versus displacement, a linear fit seems to be suitable. However, data in the upper-left side of the graph seems like they would probably deviate from a linear fit given by the rest of the data set. 
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
library(MPV)
cars<- table.b3 #load the data
plot( y~ x1, data=cars,pch=21, bg=2 )  #construct the scatterplot
```

Trying to fit a linear model confirms this suspects, as the residuals vs fitted diagnostic graph shows that a non-linear relation is present.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
carslm<-lm(y~x1, data = cars) #fit a linear model
par(mfrow=c(2,2))
plot(carslm) #plot the diagnostic
```

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
plot( y~ x1, data=cars,pch=21, bg=2 ) #re-plot the scatterplot
abline(carslm) #add the linea model
```

* Use the xyplot() function, and x11 (type of transmission) as a group variable. Is a
linear model reasonable for these data?

Using the type of transmission as a group variable highlights two different clusters of data, each of one seems to be individually more suitable for a linear fit.
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
xyplot(y~x1, data=cars, groups = x11) #construct the scatterplot using x11 as a group variable
```

* Fit the model relating y to x1 and x11 which gives two lines having possibly different
slopes and intercepts. Check the diagnostics. Are there any influential observations? Are
there any influential outliers?

Apart from the residuals of a small number of data which, resembling their distribution in the scatterplot, seems lined up, the diagnostics confirm the effectiveness of the chosen model. 
As far as outliers concern, entries number 15 and 5 are suitable candidates, in particular number 5, as it as a remarkable value of Cook's distance. 
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
carslm<-lm(y~x1+x11+x11:x1, data = cars) #fit a linear model made of two different lines
summary(carslm) #print out the results of the linear model
xyplot( y~x1, data=cars, groups = x11,    #costruct the scatterplot using x11 as a group variable
           panel = function(...) {
           panel.abline(a=carslm$coefficients[1]+carslm$coefficients[3] ,b=carslm$coefficients[2]+carslm$coefficients[4]) #add to the plote the first line of the model
           panel.abline(a=carslm$coefficients[1] ,b=carslm$coefficients[2],lty=2) #add to the plot the second line of the model
           panel.xyplot(...)
       })
par(mfrow=c(2,2))
plot(carslm) #plot the diagnostics of the model
```

* Plot the residuals against the variable x7 (number of transmission speeds), again using
x11 as a group variable. Is there anything striking about this plot?

In the plot, the group variable makes immediately stand out a single data of 0 value in the variable x11 with number of transmission speeds equal to 3. 
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
resid<-residuals(carslm) #get the values of the residuals from the model
cars<- table.b3 
xyplot(resid~x7, data=cars, groups = x11)#plot residual agains number of transmission speeds
```

A quick check shows that it correspond to observation number 5, identified previously as an outlier, leading one to think it was probably inserted wrongly in the data set.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
cars<- cars[-c(5),] #ecluding observation number 5
carslm<-lm(y~x1+x11+x11:x1, data = cars) #re-fit the dataset
resid<-residuals(carslm) #re-get the values of the residuals from the model
xyplot(resid~x7, data=cars, groups = x11) #re-plot residual agains number of transmission speeds
```




## Chapter 7

### Exercise 5

**Request**:  The data frame cuckoos holds data on the lengths and breadths of eggs of cuckoos, found in
the nests of six different species of host birds. Fit models for the regression of length on breadth
that have:
A: a single line for all six species.
B: different parallel lines for the different host species.
C: separate lines for the separate host species.
Use the anova() function to print out the sequential analysis of variance table. Which of the
three models is preferred? Print out the diagnostic plots for this model. Do they show anything
worthy of note? Examine the output coefficients from this model carefully, and decide whether
the results seem grouped by host species. How might the results be summarized for reporting
purposes?


**Solution**:

```{r}
require(DAAG)
data(cuckoos)

model1 <- lm(length ~ breadth, data = cuckoos) #model requested by A
model2 <- lm(length ~ breadth + species, data = cuckoos) #model requested by B
model3 <- lm(length ~ breadth + species + breadth:species, data = cuckoos)  #model requested by C

anova(model1, model2, model3) #compare the three model

```

The second model shows a smaller p-value. It is therefore among the three candidates, the best model.

```{r}
par(mfrow=c(2,2))
plot(model2)
```

Altogether the fit seems to work quite well, even though in the Normal Q-Q plot a bit of deviance is noticeable in the two ends of the diagonal line. 
In the plot of residual vs leverage a division between two main clusters of data is noticeable.

```{r}
summary(model2)
xyplot( length ~ breadth , data = cuckoos, groups = species,  #print a scatterplot of length against breadth, using species as a group variable
       panel = function(...) {  #ad the linear models corresponding to each species
           panel.abline(a=model2$coefficients[1] ,b=model2$coefficients[2] )
           panel.abline(a=model2$coefficients[1]+model2$coefficients[3] ,b=model2$coefficients[2],lty=2, col="red")
           panel.abline(a=model2$coefficients[1]+model2$coefficients[4] ,b=model2$coefficients[2],lty=3, col="blue")
           panel.abline(a=model2$coefficients[1]+model2$coefficients[5] ,b=model2$coefficients[2],lty=4, col="purple" )
           panel.abline(a=model2$coefficients[1]+model2$coefficients[6] ,b=model2$coefficients[2],lty=5, col="yellow" )
           panel.abline(a=model2$coefficients[1]+model2$coefficients[7] ,b=model2$coefficients[2],lty=6, col="pink" )
           panel.xyplot(...)
       })
```

We can notice that the F statistic is nerveless quite big, so maybe even though the model is the best among the three tested, is not necessary the best model that we can use to fit our data.

Among all the fitted models, three lines are almost overlapped, making a separate fit for each independent species redundant. To summarize, as far as a linear model con detect, three species (sparrow, pipit and wagtail) shows no significant differences in their distributions, so they can be considered as one.




### Exercise 7

**Request**: Apply polynomial regression to the seismic timing data in the data frame geophones. Specifically, check the fits of linear, quadratic, cubic, and quartic (degree = 4) polynomial estimates
of the expected thickness as a function of distance. What do you observe about the fitted quartic curve? Do any of the fitted curves capture the curvature of the data in the region where
distance is large?

**Solution:**

We fit the requested model and plot the curves

```{r, include=TRUE, eval=TRUE}
linearlm <- lm(thickness ~ distance, data=geophones)
quadraticlm <- lm(thickness ~ poly(distance, degree=2, raw=TRUE), data=geophones)
cubiclm <- lm(thickness ~ poly(distance, degree=3, raw=TRUE), data=geophones)
quarticlm <- lm(thickness ~ poly(distance, degree=4, raw=TRUE), data=geophones)

plot(thickness ~ distance, data=geophones)
lines(geophones$distance , linearlm$fitted.values, lty =1 , col = "blue")
lines(geophones$distance , quadraticlm$fitted.values, lty =1 , col = "red")
lines(geophones$distance , cubiclm$fitted.values, lty =1 , col = "green")
lines(geophones$distance , quarticlm$fitted.values, lty =1 , col = "yellow")
legend("bottomleft", inset=0.02, c("linear regression", "quadratic regression", "cubic regression", "quartic regression"), col=c("blue", "red", "green", "yellow"), cex=0.8, lty=1)
```

From the plot we can notice that the quartic curve (in yellow) seems to be the only one able to capture the curvature of the data in the region where *distance* is large. Infact the values of *thickness* seem to increase again and the quartic curve is the only one which gets and represents this final trend. 

If we compute the value of AIC for every model we obtain again that the quartic model seems to be the best one among these four models.

```{r, include=FALSE, eval=TRUE}
AIC(linearlm)
AIC(quadraticlm)
AIC(cubiclm)
AIC(quarticlm)
```

|Model|AIC|
|--------|--------|
|   linear  |  355.9209  |
| quadratic |  316.4528  |
|   cubic   |  318.1856  |
|  quartic  |  290.2589  |

## Chapter 11

### Exercise 5

**Request**:  This exercise will compare alternative measures of accuracy from *randomForest()* runs.
First, 16 rows of *biops* where data (on V6) is missing will be omitted:
```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
library(MASS)
library(randomForest)
sapply(biopsy, function(x)sum(is.na(x)))
biops <- na.omit(biopsy[,-1]) ## Column 1 is ID
```

* Compare repeated randomForest() runs: repeated runs, note variation in OOB accuracy.

* Compare OOB accuracies with test set accuracies: plot test set accuracies against OOB accuracies. Add the line y = x to the plot. Is there any consistent difference in the accuracies? Given a random training/test split, is there any
reason to expect a consistent difference between OOB accuracy and test accuracy?

* Calculate the error rate for the training data: explain why use of the training data for testing leads to an error rate that is zero.

**Solution**:

First we are asked to note the variation in OOB accuracy. We can do it by running this code:
```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
for (i in 1:10) {
  biops.rf <- randomForest(class ~ ., data=biops)
  OOBerr <- mean(biops.rf$err.rate[,"OOB"])
  print(paste(i, ": ", round(OOBerr, 4), sep=""))
  print(round(biops.rf$confusion,4))
}
```

As first thing we need to note that the code given by the book prints the errors and not the accuracy, but analyizing one instead of the other in this case it is the same since $accuracy = 1 - error$
As we expected, the variation of the OOB errors it's really small, infact the errors are always between 0.028 and 0.031, this is due to the fact that we are training at each iteration a random forest on the same data. The small variation is caused only by the randomity of the random forest itself.

Then we are asked to compare test set accuracy against OOB accuracy. To do it we first need to compute the test set accuracy, and we can do it by running this code:

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
oobErr <- rep(0,100)
testErr <- rep (0,100)
for(i in 1:100){
  trRows <- sample(1:dim(biops)[1], size=round(dim(biops)[1]/2))
  biops.rf <- randomForest(class  ~ ., data=biops[trRows, ], xtest=biops[-trRows,-10], ytest=biops[-trRows,10])
  oobErr[i] <- mean(biops.rf$err.rate[,"OOB"])
  testErr[i] <- mean(biops.rf$test$err.rate[,"Test"])
  #print(round(c(oobErr,testErr),4))
}
plot(testErr,oobErr)
lines(testErr, y=testErr)
```

In this case we have something more interesting: here we are training at each iteration a random forest on a different sample of the original data, so it is more expectable that the errors show a bigger variation. What was unexpected instead is the negative correlation between the OOB errors and the Test ones, infact from the plot it is clear that they're distributed on the line y = -x. We would have bet on positive correlation.

As last point we are asked to calculate the error rate for the training data.

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
randomForest(class  ~ ., data=biops, xtest=biops[,-10], ytest=biops[,10])
```

The error rate is really small due to the fact that we are testing the model with the same data used for training, in other words the model will certainly classify the data correctly because it is perfectly fitted on those data (overfitted).


## Exercise 1

**Request**: The General Social Survey (GSS) has been conducted in the United States every two years since 1972.

* Go to the GSS website and download the data. Consider a question of interest that was asked in many rounds of the survey and convert it to a binary outcome, if it is not binary already. Decide how you will handle nonresponse in your analysis.

* Make a graph of the average response of this binary variable over time, each year giving ±1 standard error bounds.

* Set up a logistic regression of this outcome variable given predictors for age, sex, education, and ethnicity. Fit the model separately for each year that the question was asked, and make a grid of plots with the time series of coefficient estimates ± standard errors over time.

* Fit a regression tree, report results, and comment.

* Discuss the results and how you might want to expand your model to answer some social science question of interest.

**Solution**:

First we have downloaded the data from the GSS website and we choosed to analyze the variable *wrkstat* which indicates the actual status of a person (student, full time worker, part time worker, unemployed, retired, keeping house, ...). We have transformed the values of the variable so that the value is 1 if the person is a worker (no matter in full time or part time) and 0 if the person is not a worker (student, unemployed, retired, keeping house, ...). For this variable there were 19 missing value and so, due to the very small amount of NA values compared to the dataset dimension (over than 60000 data) we decided to omit the missing values. We noticed that the variables *sex* and *race* were saved in the dataset as double even if both of them are qualitative variables, so we transformed them in factors.

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
setwd("~/Downloads")
library(haven)
gss_data <- read_sav("gss_data.sav")

#we make the problem a binary problem
gss_data$WRKSTAT[gss_data$WRKSTAT < 3] <- 1
gss_data$WRKSTAT[gss_data$WRKSTAT >= 3 & gss_data$WRKSTAT < 9] <- 0

#we omit all the data with no value for the response variable
gss_data <- na.omit(gss_data , "WRKSTAT")
gss_data <-na.omit(gss_data, "AGE")
gss_data <-na.omit(gss_data, "EDUC")

gss_data$SEX <- as.factor(gss_data$SEX)
gss_data$RACE <- as.factor(gss_data$RACE)
levels(gss_data$SEX)[levels(gss_data$SEX)=="1"] <- "M"
levels(gss_data$SEX)[levels(gss_data$SEX)=="2"] <- "F"
levels(gss_data$RACE)[levels(gss_data$RACE) == "1"] <- "Bianco"
levels(gss_data$RACE)[levels(gss_data$RACE) == "2"] <- "Nero"
levels(gss_data$RACE)[levels(gss_data$RACE) == "3"] <- "Altro"
```

Then we plot the average of the binary response variable *wrkstat* over time (black points in the plot) with the standard error (dashed lines). We can see that it seems to be a trend among years.

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
#we create a vector where to store all the means and the standard errors for every year
mean_wrkstat <- rep(0,32)
sd_wrkstat <- rep(0,32)
index=1

#we fill the vectors with the mean and the sd for every year
for (i in unique(gss_data$YEAR)){
  mean_wrkstat[index]<- mean(gss_data$WRKSTAT[gss_data$YEAR == i])
  sd_wrkstat[index] <- sqrt((mean_wrkstat[index]*(1-mean_wrkstat[index]))/length(gss_data$WRKSTAT[gss_data$YEAR == i]))
  index = index +1
}

#we plot the results
plot(unique(gss_data$YEAR), mean_wrkstat, ylim=c(0.485, 0.675), pch=19, ylab="Year", xlab="Mean Wrkstat", main="Mean of wrkstat among years")
points(unique(gss_data$YEAR), mean_wrkstat+sd_wrkstat, pch="-")
points(unique(gss_data$YEAR), mean_wrkstat-sd_wrkstat, pch="-")

for (i in unique(gss_data$YEAR)){
  segments(unique(gss_data$YEAR), mean_wrkstat-sd_wrkstat, unique(gss_data$YEAR), mean_wrkstat+sd_wrkstat, lty=3)
}
```

The third point of the exercise asked to set up a logistic regression of *wrkstat* given predictors for age, sex, education, and ethnicity, fitting the model separately for each year that the question was asked, and make a grid of plots with the time series of coefficient estimates ± standard errors over time. Here there's the resulting plots.

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE, fig.width=12, fig.height=10}
index = 1
#we create enough vector to store all the estimate values of the coefficients and thei standard errors
est_coef_intercept = rep(0,32); est_coef_age = rep(0,32); est_coef_sex_f = rep(0,32); est_coef_educ = rep(0,32); est_coef_race_nero = rep(0,32); est_coef_race_altro = rep(0,32); est_se_intercept = rep(0,32); est_se_age = rep(0,32); est_se_sex_f = rep(0,32); est_se_educ = rep(0,32); est_se_race_nero = rep(0,32); est_se_race_altro = rep(0,32)

#we fit the model for every year and we fill the vector
for (i in unique(gss_data$YEAR)){
  gss_data_year<- subset(gss_data, YEAR == i, select = c(EDUC , AGE , SEX , RACE , WRKSTAT))
  fit_year <- glm(WRKSTAT ~ AGE + SEX + EDUC + RACE, family = binomial, data=gss_data_year)
  coef <- as.data.frame(summary(fit_year)[12])
  est_coef_intercept [index] = fit_year$coefficients[1]
  est_coef_age [index] = fit_year$coefficients[2]
  est_coef_sex_f [index] = fit_year$coefficients[3]
  est_coef_educ [index] = fit_year$coefficients[4]
  est_coef_race_nero [index] = fit_year$coefficients[5]
  est_coef_race_altro [index] = fit_year$coefficients[6]
  est_se_intercept [index] = coef$coefficients.Std..Error[1]
  est_se_age [index] = coef$coefficients.Std..Error[2]
  est_se_sex_f [index] = coef$coefficients.Std..Error[3]
  est_se_educ [index] = coef$coefficients.Std..Error[4]
  est_se_race_nero [index] = coef$coefficients.Std..Error[5]
  est_se_race_altro [index] = coef$coefficients.Std..Error[6]
  assign(paste0("fit_" , i), fit_year)
  index = index + 1 
}

par(mfrow=c(3,2))
plot(unique(gss_data$YEAR), est_coef_intercept, ylim=c(-0.15,2.8),pch=19, xlab="Year", ylab="Estimated coefficients", main="Estimated coefficients of intercept among years")
points(unique(gss_data$YEAR), est_coef_intercept + est_se_intercept, pch="-")
points(unique(gss_data$YEAR), est_coef_intercept - est_se_intercept, pch="-")
for (i in unique(gss_data$YEAR)){
  segments(unique(gss_data$YEAR), est_coef_intercept-est_se_intercept, unique(gss_data$YEAR), est_coef_intercept+est_se_intercept, lty=3)
}

plot(unique(gss_data$YEAR), est_coef_age, pch=19, ylim=c(-0.065, -0.017), xlab="Year", ylab="Estimated coefficients", main="Estimated coefficients of age among years")
points(unique(gss_data$YEAR), est_coef_age + est_se_age, pch="-")
points(unique(gss_data$YEAR), est_coef_age - est_se_age, pch="-")
for (i in unique(gss_data$YEAR)){
  segments(unique(gss_data$YEAR), est_coef_age-est_se_age, unique(gss_data$YEAR), est_coef_age+est_se_age, lty=3)
}

plot(unique(gss_data$YEAR), est_coef_sex_f, pch=19, ylim=c(-1.76, -0.3), xlab="Year", ylab="Estimated coefficients", main="Estimated coefficients of sex among years")
points(unique(gss_data$YEAR), est_coef_sex_f + est_se_sex_f, pch="-")
points(unique(gss_data$YEAR), est_coef_sex_f - est_se_sex_f, pch="-")
for (i in unique(gss_data$YEAR)){
  segments(unique(gss_data$YEAR), est_coef_sex_f-est_se_sex_f, unique(gss_data$YEAR), est_coef_sex_f+est_se_sex_f, lty=3)
}

plot(unique(gss_data$YEAR), est_coef_educ, pch=19, ylim=c(0.07, 0.24), xlab="Year", ylab="Estimated coefficients", main="Estimated coefficients of educ among years")
points(unique(gss_data$YEAR), est_coef_educ + est_se_educ, pch="-")
points(unique(gss_data$YEAR), est_coef_educ - est_se_educ, pch="-")
for (i in unique(gss_data$YEAR)){
  segments(unique(gss_data$YEAR), est_coef_educ-est_se_educ, unique(gss_data$YEAR), est_coef_educ+est_se_educ, lty=3)
}

plot(unique(gss_data$YEAR), est_coef_race_nero, pch=19, ylim=c(-0.68, 0.52), xlab="Year", ylab="Estimated coefficients", main="Estimated coefficients of race_nero among years")
points(unique(gss_data$YEAR), est_coef_race_nero + est_se_race_nero, pch="-")
points(unique(gss_data$YEAR), est_coef_race_nero - est_se_race_nero, pch="-")
for (i in unique(gss_data$YEAR)){
  segments(unique(gss_data$YEAR), est_coef_race_nero-est_se_race_nero, unique(gss_data$YEAR), est_coef_race_nero+est_se_race_nero, lty=3)
}

plot(unique(gss_data$YEAR), est_coef_race_altro, pch=19, ylim=c(-1.9, 2), xlab="Year", ylab="Estimated coefficients", main="Estimated coefficients of race_altro among years")
points(unique(gss_data$YEAR), est_coef_race_altro + est_se_race_altro, pch="-")
points(unique(gss_data$YEAR), est_coef_race_altro - est_se_race_altro, pch="-")
for (i in unique(gss_data$YEAR)){
  segments(unique(gss_data$YEAR), est_coef_race_altro-est_se_race_altro, unique(gss_data$YEAR), est_coef_race_altro+est_se_race_altro, lty=3)
}
par(mfrow=c(1,1))
```

Then we fitted a regression tree using the one-standard-deviation-rule.

```{r, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
library(rpart)
gss_tree <- rpart(WRKSTAT ~ AGE + SEX + EDUC + RACE + YEAR, method="class", data=gss_data, cp=0) 
best_cp <- gss_tree$cptable[which.min(gss_tree$cptable[,"xerror"]),]
sd_rule <- best_cp["xerror"]+best_cp["xstd"]
cptable_sd_rule <- gss_tree$cptable[gss_tree$cptable[,"xerror"]<=sd_rule,]
best_cp_sd <- cptable_sd_rule[which.min(cptable_sd_rule[,"nsplit"]),]

tree.pruned.sd <- prune(gss_tree, cp=best_cp_sd[1])
plot(tree.pruned.sd, margin=0.1)
text(tree.pruned.sd)
printcp(tree.pruned.sd)
```

Even if the tree seems to be not too much accurate (the Absolute cross validation error is around 0.25, which means that the error rate is ~25%) it is still interesting to have a graphical representation of how each variable affect the output. 
For example in the first node we can see that everyone who's older than 63 years is certainly not working, that's because they're retired. Otherwise the next splits are often a combination of sex, education and also the year in which the data where observed, which means that the importance of a variable changes among the years. Note that "SEX=b" means Female, and that in the terminal node 0 means "not worker" while 1 means "worker".


Regarding how we can expand the model, certainly the first thing could be adding explanatory variables, e.g. 
the country of the interviewed, or more specifically the region of the country, in that way we could draw a sort of map of the distribution of the labor status among the years and across the country. Another important change could be to train a model which fits the data better then a simple tree, e.g. random forest, so that we could have a better prediction.


## Exercise 2

**Request**: The file risky_behaviors.dta contains data from a randomized trial targeting couples at high risk of HIV infection. The intervention provided counseling sessions regarding practices that could reduce their likelihood of contracting HIV. Couples were randomized either to a control group (women_alone = no woman counselling and couples = no couple counselling), a group in which just the woman participated (women_alone = only woman counselling and couples = no couple counselling), or a group in which both members of the couple participated (women_alone = no woman counselling and couples = couple counselling). Some variables are collected at the beginning of the study:

*sex the gender of the interviewed member of the couple that reported sex acts;
*bs_hiv if the interviewed member was HIV-positive before the study;
*bupacts the number of unprotected sex acts before the study.

One of the outcomes examined after three months was “number of unprotected sex acts at the end of the study” fupacts.
```{r}
#load the data and set up for the analysis
library(foreign)
data <- read.dta("risky_behaviors.dta")
data$couples <- factor(data$couples) ## counselling session to couples
levels(data$couples) <-  c("no couple counselling", "couple conselling")
data$women_alone <- factor(data$women_alone) ## counselling session only to women
levels(data$women_alone) <-  c("no woman counselling","only woman counselling")
data$fupacts <- round(data$fupacts)
summary(data)
```
*Model this outcome as a function of treatment assignment using a Poisson regression. Does the model fit well? Is there evidence of overdispersion?
*Next extend the model to include pre-treatment measures of the outcome and the additional pre-treatment variables included in the dataset. Does the model fit well? Is there evidence of overdispersion?
*Fit a negative binomial (overdispersed Poisson) model. What do you conclude regarding effectiveness of the intervention?
*These data include responses from both men and women from the participating couples. Does this give you any concern with regard to our modeling assumptions?



**Solution**:

As is clear from all dignostics, the model only fit poorly the dataset: residuals are not evenly distributed in the first plot, depart themselves clearly in the Normal Q-Q plot, and show a great deal of outlier in the leverage one. The ratio between the residual deviance and the number of degrees of freedom suggest the presence of overdispersion. Triying to fit a quasi-poisson model, returns a dispersion parameter of around 44, proving the great presence of overdispersion in this model.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
#load the data and set up for the analysis
data <- read.dta("risky_behaviors.dta")
data$couples <- factor(data$couples) ## counselling session to couples
levels(data$couples) <-  c("no couple counselling", "couple conselling")
data$women_alone <- factor(data$women_alone) ## counselling session only to women
levels(data$women_alone) <-  c("no woman counselling","only woman counselling")
data$fupacts <- round(data$fupacts)
summary(data)
#plot a scatterplot of number of unprotected sex acts against treatment assignment
xyplot( fupacts ~ women_alone+couples , data = data)
#fit a poisson distibution
fit.1 <- glm(fupacts ~ women_alone+couples, family=poisson, data=data)
#check the performance of the poisson model and its diagnostics
summary(fit.1)
par(mfrow=c(2,2))
plot(fit.1)
#fit a quasi-poisson distibution
fit.2 <- glm(fupacts ~ women_alone+couples, family=quasipoisson, data=data)
#check the performance of the quasi-possion model
summary(fit.2)

```

* Next extend the model to include pre-treatment measures of the outcome and the additional pre-treatment variables included in the dataset. Does the model fit well? Is there evidence of overdispersion?

The diagnostic plots is again not so good, even though this second time they return more promising results. Event tough the all general fit worsen a bit, showing residuals distributed less evenly, as well as an unbalanced Scale-Location plot, the great number of outliers has been mitigated, but still occurring. Once again a quasi-poisson fit model returns a dispersion parameter of about 30: greatly improved from before, but still far away from the ideal gol of 1.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
fit.1 <- glm(fupacts ~ women_alone+couples+sex+bs_hiv+bupacts, family=poisson, data=data) #fit a poisson distibution with all parameters
summary(fit.1) #check the performance of the poisson model and its diagnostics
par(mfrow=c(2,2))
plot(fit.1)
fit.2 <- glm(fupacts ~ women_alone+couples+sex+bs_hiv+bupacts, family=quasipoisson, data=data) #fit a quasi-poisson distibution
summary(fit.2) #check the performance of the quasi-possion model
```

* Fit a negative binomial (overdispersed Poisson) model. What do you conclude regarding effectiveness of the intervention?

In the normalized residuals, we see a net improvement in the plotted distribution. This can be justified by the decrease in overdispersion, as the value of residual distance is now much closer to the number of degrees of freedom. An improvement in the Scale-Location plot is also noticeable, as the distribution of the Parson residuals are again resembling more a line closer to 0.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
fit.1 <- glm.nb(fupacts ~ women_alone+couples+sex+bs_hiv+bupacts,  data=data) #fit a negative binomial with all parameters
summary(fit.1) #check the performance of the negative binomial model and its diagnostics
par(mfrow=c(2,2))
plot(fit.1)
```

* These data include responses from both men and women from the participating couples. Does this give you any concern with regard to our modeling assumptions?

Using a Poisson regression, we are assuming that every observation is independent. As we are including both member of each couple, however, it is reasonable to assume that their value of unprotected sex acts, (before and after the treatment) will match, or at least be similar. 
